{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent - Implement\n",
    "\n",
    "This is for ONE output layer (no hidden layers)\n",
    "\n",
    "Implement gradient descent and train the network on the admissions data. Your goal here is to train the network until you reach a minimum in the mean square error (MSE) on the training set. You need to implement:\n",
    "\n",
    "- The network output: output.\n",
    "- The output error: error.\n",
    "- The error term: error_term.\n",
    "- Update the weight step: del_w +=.\n",
    "- Update the weights: weights +=.\n",
    "\n",
    "Instead of the SSE, use the mean of the square errors (MSE), divide by the number of records in the data, mm to take the average.\n",
    "\n",
    "Here's the general algorithm for updating the weights with gradient descent:\n",
    "\n",
    "Set the **weight step** to zero: \\Delta w_i = 0\n",
    "\n",
    "- For each record in the training data:\n",
    "\n",
    "Make a forward pass through the network, calculating the output \\hat y = f(\\sum_i w_i x_i) \n",
    "\n",
    "Calculate the error term for the output unit, \\delta = (y - \\hat y) * f'(\\sum_i w_i x_i)\n",
    "\n",
    "Update the weight step \\Delta w_i = \\Delta w_i + \\delta x_i\n",
    "\n",
    "Update the weights w_i = w_i + \\eta \\Delta w_i / m where \\etaÎ· is the learning rate and mm is the number of records. Here we're averaging the weight steps to help reduce any large variations in the training data.\n",
    "\n",
    "Repeat for ee epochs.\n",
    "\n",
    "You can also update the weights on each record instead of averaging the weight steps after going through all the records.\n",
    "\n",
    "#### Numpy\n",
    "\n",
    "**First, you'll need to initialize the weights of the model.\n",
    "\n",
    "Want these to be small such that the input to the sigmoid is in the linear region near 0 and not squashed at the high and low ends. It's also important to initialize them randomly so that they all have different starting values and diverge, breaking symmetry. So, we'll initialize the weights from a normal distribution centered at 0. A good value for the scale is 1/\\sqrt{n}\t  where nn is the number of input units. This keeps the input to the sigmoid low for increasing numbers of input units.\n",
    "\n",
    "*weights = np.random.normal(scale=1/n_features**.5, size=n_features)\n",
    "\n",
    "NumPy provides a function np.dot() that calculates the dot product of two arrays, which conveniently calculates hh for us. The dot product multiplies two arrays element-wise, the first element in array 1 is multiplied by the first element in array 2, and so on. Then, each product is summed.\n",
    "\n",
    "*output_in = np.dot(weights, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # Note: We haven't included the h variable from the previous\n",
    "        #       lesson. You can add it if you want, or you can calculate\n",
    "        #       the h together with the output\n",
    "        \n",
    "        # helper calc\n",
    "        h = np.dot(x, weights)\n",
    "        \n",
    "        # TODO: Calculate the output\n",
    "        output = sigmoid(h)\n",
    "\n",
    "        # DONE: Calculate the error\n",
    "        error = y - output\n",
    "        \n",
    "        # DONE: Calculate the error term\n",
    "# Note: The sigmoid_prime function calculates sigmoid(h) twice,\n",
    "#       You can make this code more efficient by calculating the derivative directly\n",
    "#       rather than calling sigmoid_prime.\n",
    "        error_term = error * output * (1 - output)        \n",
    "\n",
    "        # DONE: Calculate the change in weights for this sample\n",
    "        #       and add it to the total weight change\n",
    "        del_w += error_term * x\n",
    "\n",
    "    # TODO: Update weights using the learning rate and the average change in weights\n",
    "    weights += learnrate * del_w / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
